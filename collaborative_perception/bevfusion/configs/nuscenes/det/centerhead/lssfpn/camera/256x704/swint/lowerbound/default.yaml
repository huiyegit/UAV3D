model:
  encoders:
    camera:
      backbone:
        type: SwinTransformer
        embed_dims: 96
        depths: [2, 2, 6, 2]
        num_heads: [3, 6, 12, 24]
        window_size: 7
        mlp_ratio: 4
        qkv_bias: true
        qk_scale: null
        drop_rate: 0.
        attn_drop_rate: 0.
        drop_path_rate: 0.2
        patch_norm: true
        out_indices: [1, 2, 3]
        with_cp: false
        convert_weights: true
        init_cfg:
          type: Pretrained
          checkpoint: https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth
      com:
        agent_num: 5
        in_channels: 80
        out_channels: 80
        type: lowerbound
      neck:
        type: GeneralizedLSSFPN
        in_channels: [192, 384, 768]
        out_channels: 256
        start_level: 0
        num_outs: 3
        norm_cfg:
          type: BN2d
          requires_grad: true
        act_cfg:
          type: ReLU
          inplace: true
        upsample_cfg:
          mode: bilinear
          align_corners: false      
      vtransform:
        in_channels: 256
        out_channels: 80
        feature_size: ${[image_size[0] // 8, image_size[1] // 8]}
        xbound: [-102.4, 102.4, 0.8]
        ybound: [-102.4, 102.4, 0.8]
        zbound: [-20.0, 20.0, 40.0]
        dbound: [1.0, 181.0, 1]
        downsample: 2
  decoder:
    backbone:
      type: GeneralizedResNet
      in_channels: 80
      blocks:
        - [2, 128, 2]
        - [2, 256, 2]
        - [2, 512, 1]
    neck:
      type: LSSFPN
      in_indices: [-1, 0]
      in_channels: [512, 128]
      out_channels: 256
      scale_factor: 2

optimizer:
  paramwise_cfg:
    custom_keys:
      absolute_pos_embed:
        decay_mult: 0
      relative_position_bias_table:
        decay_mult: 0
      encoders.camera.backbone:
        lr_mult: 0.1


lr_config:
  policy: cyclic
  target_ratio: 5.0
  cyclic_times: 1
  step_ratio_up: 0.4

momentum_config:
  policy: cyclic
  cyclic_times: 1
  step_ratio_up: 0.4

data:
  samples_per_gpu: 1

object_classes:
  - car

# dataset_root: /home/hye/yh/dataset/gsu-uav/test2/4_scene64/
# dataset_root: /home/hye/yh/dataset/gsu-uav/gsu_2_1/town_all/
# dataset_root: ./data/uav3d/v1.0-mini/
dataset_root: ./data/uav3d/v1.0/

#input_modality:
#  use_lidar: true
#  use_camera: true
#  use_radar: false
#  use_map: false
#  use_external: false

#input_modality:
#  use_lidar: false
#  use_camera: true
#  use_radar: false
#  use_map: false
#  use_external: false


input_modality:
  use_lidar: false
  use_camera: true
  use_radar: false
  use_map: false
  use_external: false

train_pipeline:
  -
    type: LoadMultiViewImageFromFiles
    to_float32: true
#  -
#    type: LoadPointsFromFile
#    coord_type: LIDAR
#    load_dim: ${load_dim}
#    use_dim: ${use_dim}
#    reduce_beams: ${reduce_beams}
#    load_augmented: ${load_augmented}
#  -
#    type: LoadPointsFromMultiSweeps
#    sweeps_num: 9
#    load_dim: ${load_dim}
#    use_dim: ${use_dim}
#    reduce_beams: ${reduce_beams}
#    pad_empty_sweeps: true
#    remove_close: true
#    load_augmented: ${load_augmented}
  -
    type: LoadAnnotations3D
    with_bbox_3d: true
    with_label_3d: true
    with_attr_label: False
#  -
#    type: ObjectPaste
#    stop_epoch: ${gt_paste_stop_epoch}
#    db_sampler:
#      dataset_root: ${dataset_root}
#      info_path: ${dataset_root + "nuscenes_dbinfos_train.pkl"}
#      rate: 1.0
#      prepare:
#        filter_by_difficulty: [-1]
#        filter_by_min_points:
#          car: 5
#          truck: 5
#          bus: 5
#          trailer: 5
#          construction_vehicle: 5
#          traffic_cone: 5
#          barrier: 5
#          motorcycle: 5
#          bicycle: 5
#          pedestrian: 5
#      classes: ${object_classes}
#      sample_groups:
#        car: 2
#        truck: 3
#        construction_vehicle: 7
#        bus: 4
#        trailer: 6
#        barrier: 2
#        motorcycle: 6
#        bicycle: 6
#        pedestrian: 2
#        traffic_cone: 2
#      points_loader:
#        type: LoadPointsFromFile
#        coord_type: LIDAR
#        load_dim: ${load_dim}
#        use_dim: ${use_dim}
#        reduce_beams: ${reduce_beams}
  -
    type: ImageAug3D
    final_dim: ${image_size}
    resize_lim: ${augment2d.resize[0]}
    bot_pct_lim: [0.0, 0.0]
    rot_lim: ${augment2d.rotate}
    rand_flip: true
    is_train: true
  -
    type: GlobalRotScaleTrans
    resize_lim: ${augment3d.scale}
    rot_lim: ${augment3d.rotate}
    trans_lim: ${augment3d.translate}
    is_train: true
#  -
#    type: LoadBEVSegmentation
#    dataset_root: ${dataset_root}
#    xbound: [-50.0, 50.0, 0.5]
#    ybound: [-50.0, 50.0, 0.5]
#    classes: ${map_classes}
  -
    type: RandomFlip3D
#  -
#    type: PointsRangeFilter
#    point_cloud_range: ${point_cloud_range}
  -
    type: ObjectRangeFilter
    point_cloud_range: ${point_cloud_range}
  -
    type: ObjectNameFilter
    classes: ${object_classes}
  -
    type: ImageNormalize
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
  -
    type: GridMask
    use_h: true
    use_w: true
    max_epoch: ${max_epochs}
    rotate: 1
    offset: false
    ratio: 0.5
    mode: 1
    prob: ${augment2d.gridmask.prob}
    fixed_prob: ${augment2d.gridmask.fixed_prob}
#  -
#    type: PointShuffle
  -
    type: DefaultFormatBundle3D
    classes: ${object_classes}
  -
    type: Collect3D
    keys:
      - img
#      - points
      - gt_bboxes_3d
      - gt_labels_3d
#      - gt_masks_bev
    meta_keys:
      - camera_intrinsics
      - camera2ego
      - lidar2ego
      - lidar2camera
      - camera2lidar
      - lidar2image
      - img_aug_matrix
      - lidar_aug_matrix

test_pipeline:
  -
    type: LoadMultiViewImageFromFiles
    to_float32: true
#  -
#    type: LoadPointsFromFile
#    coord_type: LIDAR
#    load_dim: ${load_dim}
#    use_dim: ${use_dim}
#    reduce_beams: ${reduce_beams}
#    load_augmented: ${load_augmented}
#  -
#    type: LoadPointsFromMultiSweeps
#    sweeps_num: 9
#    load_dim: ${load_dim}
#    use_dim: ${use_dim}
#    reduce_beams: ${reduce_beams}
#    pad_empty_sweeps: true
#    remove_close: true
#    load_augmented: ${load_augmented}
  -
    type: LoadAnnotations3D
    with_bbox_3d: true
    with_label_3d: true
    with_attr_label: False
  -
    type: ImageAug3D
    final_dim: ${image_size}
    resize_lim: ${augment2d.resize[1]}
    bot_pct_lim: [0.0, 0.0]
    rot_lim: [0.0, 0.0]
    rand_flip: false
    is_train: false
  -
    type: GlobalRotScaleTrans
    resize_lim: [1.0, 1.0]
    rot_lim: [0.0, 0.0]
    trans_lim: 0.0
    is_train: false
#  -
#    type: LoadBEVSegmentation
#    dataset_root: ${dataset_root}
#    xbound: [-50.0, 50.0, 0.5]
#    ybound: [-50.0, 50.0, 0.5]
#    classes: ${map_classes}
#  -
#    type: PointsRangeFilter
#    point_cloud_range: ${point_cloud_range}
  -
    type: ImageNormalize
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
  -
    type: DefaultFormatBundle3D
    classes: ${object_classes}
  -
    type: Collect3D
    keys:
      - img
#      - points
      - gt_bboxes_3d
      - gt_labels_3d
#      - gt_masks_bev
    meta_keys:
      - camera_intrinsics
      - camera2ego
      - lidar2ego
      - lidar2camera
      - camera2lidar
      - lidar2image
      - img_aug_matrix
      - lidar_aug_matrix
