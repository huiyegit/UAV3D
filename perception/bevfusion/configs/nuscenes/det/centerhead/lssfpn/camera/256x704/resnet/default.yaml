model:
  encoders:
    camera:
      backbone:
        type: ResNet
        depth: 50
        # depth: 101
        num_stages: 4
        # out_indices: [0, 1, 2, 3]
        out_indices: [1, 2, 3]
        frozen_stages: 1
        norm_cfg:
          type: BN2d
          requires_grad: false
        norm_eval: True
        dcn:
          type: DCNv2
          deform_groups: 1
          fallback_on_stride: false
        stage_with_dcn: [False, False, True, True]
        # style : caffe
        init_cfg:
         type: Pretrained
         checkpoint: torchvision://resnet50
        #  checkpoint: torchvision://resnet101


      neck:
        type: SECONDFPN
        # in_channels: [256, 512, 1024, 2048]
        # out_channels: [128, 128, 256, 512]
        # upsample_strides: [0.25, 0.5, 1, 2]    
        # upsample_strides: [0.5, 1, 2, 4]

        in_channels: [512, 1024, 2048]
        out_channels: [128, 128, 256]
        upsample_strides: [1, 2, 4]

      vtransform:
        type: LSSTransform
        # in_channels: 384
        in_channels: 512
        out_channels: 256
        image_size: ${image_size}
        feature_size: ${[image_size[0] // 8, image_size[1] // 8]}
        # feature_size: ${[image_size[0] // 16, image_size[1] // 16]}
        # xbound: [-51.2, 51.2, 0.8]
        # ybound: [-51.2, 51.2, 0.8]
        # zbound: [-10.0, 10.0, 20.0]
        # dbound: [1.0, 60.0, 1.0]

        xbound: [-102.4, 102.4, 0.8]
        ybound: [-102.4, 102.4, 0.8]
        zbound: [-20.0, 20.0, 40.0]
        dbound: [1.0, 181.0, 1]
        downsample: 1
  decoder:
    backbone:
      type: GeneralizedResNet
      in_channels: 256
      blocks:
        - [2, 128, 2]
        - [2, 256, 2]
        - [2, 512, 1]
    neck:
      type: LSSFPN
      in_indices: [-1, 0]
      in_channels: [512, 128]
      out_channels: 256
      scale_factor: 1



optimizer:
  type: AdamW
  lr: 2.0e-4
  weight_decay: 0.01
  paramwise_cfg:
    custom_keys:
      absolute_pos_embed: 
        decay_mult: 0
      relative_position_bias_table:
        decay_mult: 0
      encoders.camera.backbone:
        lr_mult: 0.1

optimizer_config:
  grad_clip:
    max_norm: 35
    norm_type: 2

lr_config:
  policy: CosineAnnealing
  warmup: linear
  warmup_iters: 500
  warmup_ratio: 0.33333333
  min_lr_ratio: 1.0e-3


data:
  samples_per_gpu: 1

object_classes:
  - car


# dataset_root: /home/hye/yh/dataset/gsu-uav/test2/4_scene64/
# dataset_root: /home/hye/yh/dataset/gsu-uav/gsu_2_1/town_all/
# dataset_root: ./data/uav3d/v1.0-mini/
dataset_root: ./data/uav3d/v1.0/

#input_modality:
#  use_lidar: true
#  use_camera: true
#  use_radar: false
#  use_map: false
#  use_external: false

#input_modality:
#  use_lidar: false
#  use_camera: true
#  use_radar: false
#  use_map: false
#  use_external: false


input_modality:
  use_lidar: false
  use_camera: true
  use_radar: false
  use_map: false
  use_external: false

train_pipeline:
  -
    type: LoadMultiViewImageFromFiles
    to_float32: true
#  -
#    type: LoadPointsFromFile
#    coord_type: LIDAR
#    load_dim: ${load_dim}
#    use_dim: ${use_dim}
#    reduce_beams: ${reduce_beams}
#    load_augmented: ${load_augmented}
#  -
#    type: LoadPointsFromMultiSweeps
#    sweeps_num: 9
#    load_dim: ${load_dim}
#    use_dim: ${use_dim}
#    reduce_beams: ${reduce_beams}
#    pad_empty_sweeps: true
#    remove_close: true
#    load_augmented: ${load_augmented}
  -
    type: LoadAnnotations3D
    with_bbox_3d: true
    with_label_3d: true
    with_attr_label: False
#  -
#    type: ObjectPaste
#    stop_epoch: ${gt_paste_stop_epoch}
#    db_sampler:
#      dataset_root: ${dataset_root}
#      info_path: ${dataset_root + "nuscenes_dbinfos_train.pkl"}
#      rate: 1.0
#      prepare:
#        filter_by_difficulty: [-1]
#        filter_by_min_points:
#          car: 5
#          truck: 5
#          bus: 5
#          trailer: 5
#          construction_vehicle: 5
#          traffic_cone: 5
#          barrier: 5
#          motorcycle: 5
#          bicycle: 5
#          pedestrian: 5
#      classes: ${object_classes}
#      sample_groups:
#        car: 2
#        truck: 3
#        construction_vehicle: 7
#        bus: 4
#        trailer: 6
#        barrier: 2
#        motorcycle: 6
#        bicycle: 6
#        pedestrian: 2
#        traffic_cone: 2
#      points_loader:
#        type: LoadPointsFromFile
#        coord_type: LIDAR
#        load_dim: ${load_dim}
#        use_dim: ${use_dim}
#        reduce_beams: ${reduce_beams}
  -
    type: ImageAug3D
    final_dim: ${image_size}
    resize_lim: ${augment2d.resize[0]}
    bot_pct_lim: [0.0, 0.0]
    rot_lim: ${augment2d.rotate}
    rand_flip: true
    is_train: true
  -
    type: GlobalRotScaleTrans
    resize_lim: ${augment3d.scale}
    rot_lim: ${augment3d.rotate}
    trans_lim: ${augment3d.translate}
    is_train: true
#  -
#    type: LoadBEVSegmentation
#    dataset_root: ${dataset_root}
#    xbound: [-50.0, 50.0, 0.5]
#    ybound: [-50.0, 50.0, 0.5]
#    classes: ${map_classes}
  -
    type: RandomFlip3D
#  -
#    type: PointsRangeFilter
#    point_cloud_range: ${point_cloud_range}
  -
    type: ObjectRangeFilter
    point_cloud_range: ${point_cloud_range}
  -
    type: ObjectNameFilter
    classes: ${object_classes}
  -
    type: ImageNormalize
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
  -
    type: GridMask
    use_h: true
    use_w: true
    max_epoch: ${max_epochs}
    rotate: 1
    offset: false
    ratio: 0.5
    mode: 1
    prob: ${augment2d.gridmask.prob}
    fixed_prob: ${augment2d.gridmask.fixed_prob}
#  -
#    type: PointShuffle
  -
    type: DefaultFormatBundle3D
    classes: ${object_classes}
  -
    type: Collect3D
    keys:
      - img
#      - points
      - gt_bboxes_3d
      - gt_labels_3d
#      - gt_masks_bev
    meta_keys:
      - camera_intrinsics
      - camera2ego
      - lidar2ego
      - lidar2camera
      - camera2lidar
      - lidar2image
      - img_aug_matrix
      - lidar_aug_matrix

test_pipeline:
  -
    type: LoadMultiViewImageFromFiles
    to_float32: true
#  -
#    type: LoadPointsFromFile
#    coord_type: LIDAR
#    load_dim: ${load_dim}
#    use_dim: ${use_dim}
#    reduce_beams: ${reduce_beams}
#    load_augmented: ${load_augmented}
#  -
#    type: LoadPointsFromMultiSweeps
#    sweeps_num: 9
#    load_dim: ${load_dim}
#    use_dim: ${use_dim}
#    reduce_beams: ${reduce_beams}
#    pad_empty_sweeps: true
#    remove_close: true
#    load_augmented: ${load_augmented}
  -
    type: LoadAnnotations3D
    with_bbox_3d: true
    with_label_3d: true
    with_attr_label: False
  -
    type: ImageAug3D
    final_dim: ${image_size}
    resize_lim: ${augment2d.resize[1]}
    bot_pct_lim: [0.0, 0.0]
    rot_lim: [0.0, 0.0]
    rand_flip: false
    is_train: false
  -
    type: GlobalRotScaleTrans
    resize_lim: [1.0, 1.0]
    rot_lim: [0.0, 0.0]
    trans_lim: 0.0
    is_train: false
#  -
#    type: LoadBEVSegmentation
#    dataset_root: ${dataset_root}
#    xbound: [-50.0, 50.0, 0.5]
#    ybound: [-50.0, 50.0, 0.5]
#    classes: ${map_classes}
#  -
#    type: PointsRangeFilter
#    point_cloud_range: ${point_cloud_range}
  -
    type: ImageNormalize
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
  -
    type: DefaultFormatBundle3D
    classes: ${object_classes}
  -
    type: Collect3D
    keys:
      - img
#      - points
      - gt_bboxes_3d
      - gt_labels_3d
#      - gt_masks_bev
    meta_keys:
      - camera_intrinsics
      - camera2ego
      - lidar2ego
      - lidar2camera
      - camera2lidar
      - lidar2image
      - img_aug_matrix
      - lidar_aug_matrix




